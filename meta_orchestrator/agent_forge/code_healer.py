import os
from typing import Dict, Any

# Conditionally import OpenAI to avoid making it a hard dependency
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None


class CodeHealer:
    """
    Analyzes a failing agent's code and the context of its failure,
    then uses an LLM to generate a potential code patch to "heal" the flaw.
    """

    def __init__(self, openai_api_key: str = None) -> None:
        """
        Initializes the CodeHealer, optionally with an OpenAI API key.
        """
        if OpenAI:
            self.client = OpenAI(api_key=openai_api_key or os.environ.get("OPENAI_API_KEY"))
        else:
            self.client = None

    def generate_patch(self, source_code: str, failure_analysis: Dict[str, Any]) -> str:
        """
        Generates a code patch to fix a flaw identified in the failure analysis.

        Args:
            source_code: The full source code of the failing agent.
            failure_analysis: A dictionary containing details about the failure,
                              such as the adversarial strategy used.

        Returns:
            The new, patched source code for the agent. Returns the original
            source code if a patch cannot be generated.
        """
        print("--- Code Healer: Attempting to generate a patch ---")
        adversary_name = failure_analysis.get("adversary", "UnknownAdversary")

        # In a real implementation, this would involve a sophisticated LLM call.
        # For now, we use a placeholder to simulate the process and allow
        # the rest of an integration to be built.
        # TODO: Replace this with a real LLM call using a well-crafted prompt.
        if "ambiguityinjector" in adversary_name.lower():
            print(f"  - Diagnosed weakness to: Ambiguity. Generating patch...")
            return self._get_mock_ambiguity_patch(source_code)

        print("  - Could not determine a specific patch for this failure. No changes made.")
        return source_code

    def _get_mock_ambiguity_patch(self, original_code: str) -> str:
        """
        A placeholder that returns a hardcoded patch for ambiguity vulnerabilities.
        This simulates an LLM response.
        """
        # This mock patch adds a simple check for conflicting keywords.
        patch = """
    def run_agent(self, context: Dict[str, Any]) -> Dict[str, Any]:
        task = context.get("task", "")

        # --- PATCHED CODE START ---
        # Added by CodeHealer to handle ambiguity.
        if "AND" in task and "OR" in task:
            print(f"{self.__class__.__name__}: Detected ambiguous task, requesting clarification.")
            return {
                "status": "CLARIFICATION_NEEDED",
                "autonomy": 0.2, # Reduced autonomy as it requires help
                "cost": 0.0002,
                "response": "The task is ambiguous. Please clarify if both conditions are required or if either is sufficient."
            }
        # --- PATCHED CODE END ---

        # Original logic would follow here...
        print(f"{self.__class__.__name__}: Executing task '{task}'")
        time.sleep(0.1) # Simulate work

        return {
            "status": "SUCCESS",
            "autonomy": 0.9,
            "cost": 0.001,
            "response": f"Successfully completed task: {task}"
        }
"""
        # In a real scenario, we'd use more robust methods to replace the
        # specific function, but for this simulation, we'll replace from
        # the function definition onwards.

        func_def_start = original_code.find("def run_agent")
        if func_def_start != -1:
            return original_code[:func_def_start] + patch.strip()

        return original_code # Return original if we can't find the method


# Example Usage
if __name__ == '__main__':
    healer = CodeHealer()

    # Example of a simple agent's code that is vulnerable to ambiguity
    vulnerable_agent_code = """
import time
from typing import Dict, Any
from ..core.base_variant import AgentVariant

class SimpleVulnerableAgent(AgentVariant):
    def run_agent(self, context: Dict[str, Any]) -> Dict[str, Any]:
        task = context.get("task", "")
        print(f"{self.__class__.__name__}: Executing task '{task}'")
        time.sleep(0.1) # Simulate work

        return {
            "status": "SUCCESS",
            "autonomy": 0.9,
            "cost": 0.001,
            "response": f"Successfully completed task: {task}"
        }
"""

    failure_context = {
        "adversary": "AmbiguityInjectorAdversary",
        "description": "Agent failed to handle a task with both 'AND' and 'OR' instructions."
    }

    patched_code = healer.generate_patch(vulnerable_agent_code, failure_context)

    print("\n\n--- Original Code ---")
    print(vulnerable_agent_code)

    print("\n\n--- Patched Code ---")
    print(patched_code)

    # Verify that the patch was applied
    assert "CLARIFICATION_NEEDED" in patched_code
    print("\n\nâœ… Patch successfully applied.")